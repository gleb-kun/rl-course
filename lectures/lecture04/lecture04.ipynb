{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba32e87d-e9f4-40cc-a6be-c89d5f5173d3",
   "metadata": {},
   "source": [
    "# Лекция 04. Алгоритмы, основанные на полезностях. Алгоритм SARSA\n",
    "\n",
    "State — Action — Reward — State — Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5fdec-a7cf-4337-bf6c-7352763a786f",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f46d98-a48e-46e3-83af-e3cff42a97c5",
   "metadata": {},
   "source": [
    "<img src=\"pictures/niranjan.jpg\" alt=\"Mahesan Niranjan\" width=\"300\">\n",
    "\n",
    "Махесан Ниранджан (Mahesan Niranjan)\n",
    "\n",
    "- Rummery G. A., Niranjan M. On-line $Q$-learning using connectionistsystems. Technical Report. University of Cambrige. 1994. URL: https://www.researchgate.net/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems\n",
    "\n",
    "<img src=\"pictures/rummery.jpg\" alt=\"Rummery G. A.\" width=\"300\">\n",
    "\n",
    "Руммери Г. А. (Rummery G. A.) ???\n",
    "\n",
    "https://www.researchgate.net/scientific-contributions/G-A-Rummery-6612457\n",
    "\n",
    "<img src=\"pictures/sutton.jpg\" alt=\"Richard Satton\" width=\"300\">\n",
    "\n",
    "Ричард Саттон (Richard S. Sutton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c971e-c3b6-4ad6-88a1-f5f03d721be6",
   "metadata": {},
   "source": [
    "## $Q$-функция и $V$-функция"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddeaa3a-0541-4f9f-baaa-278e22e007f4",
   "metadata": {},
   "source": [
    "Рассмотрим"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c93d2-9673-45d1-adf7-398044f9bd54",
   "metadata": {},
   "source": [
    "$$\n",
    "Q^{\\pi}\\left(s,a\\right)=\\mathbb{E}_{s_0=s,\\, a_0=a,\\, \\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^{t}r_t\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584ede3-af89-4fc9-b9a5-2d87b998983b",
   "metadata": {},
   "source": [
    "$$\n",
    "V^{\\pi}\\left(s\\right)=\\mathbb{E}_{s_0=s,\\,\\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^{t}r_t\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce67a7f-efa9-4c57-9135-4e3c938ee514",
   "metadata": {},
   "source": [
    "Функция $Q^{\\pi}\\left(s,a\\right)$ связана с $V^{\\pi}\\left(s\\right)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592bb14-1583-4bab-a140-82fa3043ab72",
   "metadata": {},
   "source": [
    "$$\n",
    "V^{\\pi}\\left(s\\right)=\\mathbb{E}_{a\\sim\\pi\\left(s\\right)}\\left[Q^{\\pi}(s,a)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79b638-1d86-4c46-9760-3383e2355adc",
   "metadata": {},
   "source": [
    "Встает вопрос, какую же функцию лучше настраивать агенту??? $Q^{\\pi}\\left(s,a\\right)$ или $V^{\\pi}(s)$???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfff8ad-1728-4fb6-b4b8-abe327d5b708",
   "metadata": {},
   "source": [
    "**Пример.** Предположим, что выбор действия $a$ в соостоянии $s$ приводит к:\n",
    "- состоянию $s'_1$ с вознаграждением $r_1'=1$,\n",
    "- состоянию $s'_2$ с вознаграждением $r_2'=2$.\n",
    "\n",
    "Также известны полезности для $s'_1$ и $s'_2$, $V^{\\pi}\\left(s_1'\\right)=10$ и $V^{\\pi}\\left(s_2'\\right)=-10$, тогда\n",
    "$$\n",
    "r_1'+V^{\\pi}\\left(s_1'\\right)=1+10=11,\n",
    "$$\n",
    "$$\n",
    "r_2'+V^{\\pi}\\left(s_2'\\right)=2-10=-8.\n",
    "$$\n",
    "\n",
    "Тогда ожидаемая полезность\n",
    "$$\n",
    "\\mathbb{E}\\left[r+V^{\\pi}\\left(s'\\right)\\right]=\\frac{1}{2}\\left(r_1'+V^{\\pi}\\left(s'_1\\right)+r_2'+V^{\\pi}\\left(s'_2\\right)\\right)=\\frac{1}{2}\\left(11-8\\right)=1.5.\n",
    "$$\n",
    "\n",
    "Однако если пример один, то оценка $\\mathbb{E}\\left[r+V^{\\pi}\\left(s'\\right)\\right]$ будет равна $11$ или $-8$, что далеко от ожидаемого значения $1.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a1361-fddb-4011-947b-4a5d837862b2",
   "metadata": {},
   "source": [
    "## Метод временных различий (TD-обучение)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd15908-4c4a-46d4-a497-40ae7f65f522",
   "metadata": {},
   "source": [
    "В SARSA целевые значения $Q_{\\text{tar}}$ порождаются с помощью TD-обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5891a17-2171-4f20-a9bb-8d7e44c607d4",
   "metadata": {},
   "source": [
    "Основная суть использования метода — использование нейронной сети (**сети полезностей**, **value network**), которая на входе оценивает $Q$-значения заданных пар $(s,a)$.\n",
    "\n",
    "- Генерируются $\\tau$ и даются прогнозные $\\hat Q$-значения для $\\forall (s,a)$.\n",
    "- С помощью $\\tau$ генерируются целевые $Q$-значения $Q_{\\text{tar}}$.\n",
    "- Минимизируются расстояния между $\\hat Q$ и $Q_{\\text{tar}}$ с помощью стандартной регрессионной функции потреь, такой как скп.\n",
    "- Процесс повторяем многократно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f5749-efa8-4f74-b84b-deedcc1f0754",
   "metadata": {},
   "source": [
    "### Почему не метод Монте-Карло?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b56874-57d0-46c1-86ce-f103d1d9948c",
   "metadata": {},
   "source": [
    "Пусть даны $N$ траекторий $\\tau_i, i=1,2,\\ldots,N$, начиная с состояния $s$, в котором агент выбрал дейтвие $a$. Тогда оценка по методу Монте-Карло"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ae7e2-6fb0-4312-a59b-d56f1703dac2",
   "metadata": {},
   "source": [
    "$$Q^{\\pi}_{\\text{tar:MC}}{(s, a)}=\\frac{1}{N}\\sum_{i=1}^{N}R(\\tau_{i}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541072f-38d2-4179-98e2-f5ed20a4eb69",
   "metadata": {},
   "source": [
    "Отметим, что это уравнение применимо к любому набору траекторий с любой начальной парой $(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6000f5-79d4-45df-bb02-d7d67f6b951f",
   "metadata": {},
   "source": [
    "- Агенту приходится ждать конца эпизода, прежде чем можно будет воспользоваться для обучения какими-нибудь данными из него.\n",
    "- Для расчета $Q^{\\pi}_{\\text{tar:MC}}{(s, a)}$ нужны вознаграждения для остальной части траектории, начиная с $(s,a)$. Эпизоды могут состоять из большого количества временных шагов, ограниченного $T$, что задерживает обучение. Этим обучловлено использование TD-обучения для настройки $Q$-значений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7ab72-fedf-43ec-9896-96cee105b790",
   "metadata": {},
   "source": [
    "### TD-обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0853ccd-c0a4-41e3-ad8e-7087e18695a5",
   "metadata": {},
   "source": [
    "$Q$-значения для текущего временного шага могут быть определены через $Q$-значения для следующего временного шага (рекурсивно):\n",
    "$$\n",
    "Q^{\\pi}(s,a)=\\mathbb{E}_{s'\\sim p(s'| s, a),\\, r\\sim \\mathcal R (s,a, s')}\\left[r+\\gamma\\mathbb{E}_{a'\\sim\\pi(s')}\\left[Q^{\\pi}(s',a')\\right]\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfe209-814a-491d-9018-78c87348e7a1",
   "metadata": {},
   "source": [
    "<img src=\"pictures/bellman.jpg\" alt=\"Richard E. Bellman\" width=\"300\">\n",
    "\n",
    "Ричард Эрнест Беллман (Richard E. Bellman)\n",
    "\n",
    "(1920–1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930bcfd-838d-4271-ac34-8e337c783271",
   "metadata": {},
   "source": [
    "Пусть существует нейронная сеть $Q_{\\theta}$ представляющая $Q$-функцию. В TD обучении $Q^{\\pi}_{\\text{tar}}{(s_t,a_t)}$ через оценку правой части уравнения Беллмана с помощью $Q_{\\theta}$. На каждой итерации обучения значение $\\hat Q^{\\pi}_{\\text{tar}}{(s_t,a_t)}$ обновляется и $\\hat Q^{\\pi}_{\\text{tar}}{(s_t,a_t)}\\rightarrow Q^{\\pi}_{\\text{tar}}{(s_t,a_t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6bf31-8636-4b8d-820d-8cf745bddfed",
   "metadata": {},
   "source": [
    "### Нюансы\n",
    "\n",
    "Использование уравнения Беллмана для построения $Q^{\\pi}_{\\text{tar}}{(s_t,a_t)}$ сопрежено с некоторыми проблемами:\n",
    "- Мы применяем два мат ожидания:\n",
    "    - внешнее $\\mathbb{E}_{s'\\sim p(s' | s, a),\\, r\\sim \\mathcal {R} (s,a,s')} [\\,...]$ по следующим состояниям и вознаграждениям.\n",
    "      Используем только один параметр для оценки распределения по следующим состояниям $s'$ и вознаграждениям $r$, это дает возможность отбросить внешнее мат ожидание, тогда\n",
    "      $$\n",
    "      Q^{\\pi}(s,a)=r+\\gamma\\mathbb{E}_{a'\\sim\\pi(s')} [Q^{\\pi}(s',a')].\n",
    "      $$\n",
    "    - внутреннее $\\mathbb{E}_{a'\\sim \\pi (s')}[\\, ...]$ по действиям.\n",
    "      $$\n",
    "      Q^{\\pi}(s,a)\\approx r+\\gamma Q^{\\pi}(s',a')=Q^{\\pi}_{\\text{tar:SARSA}}(s,a).\n",
    "      $$\n",
    "      DQN:\n",
    "      $$\n",
    "      Q^{\\pi}(s,a)\\approx r+ \\max_{a'_{i}}Q^{\\pi}(s',a'_i)=Q^{\\pi}_{\\text{tar:DQN}}(s,a).\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b8b15-47a1-4bee-849b-bf5a3aa4e00a",
   "metadata": {},
   "source": [
    "- Extented SARSA\n",
    "  - Саттон Р. С., Барто Э. Дж. Обучение с подкреплением: Введение. 2-е изд. М.: ДМК Пресс, 2020. 552 с."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae6f01-d522-419b-bd31-1b9b5ccdca76",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71760dac-70c6-4f2f-8e49-970181bcacd0",
   "metadata": {},
   "source": [
    "Примера пока что нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922090f-25fe-4b75-9534-2eead33a689a",
   "metadata": {},
   "source": [
    "### Выбор действий в SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a36d9-0df5-4378-8f69-0c3749a99f2a",
   "metadata": {},
   "source": [
    "1. Инициализируем случайным образом нейронную сеть с параметром $\\theta$, которая представляет $Q$-функцию и обозначена $Q^{\\pi}(s,a;\\theta)$.\n",
    "2. Повторяем следующие шаги до тех пор, пока агент не перестанет улучшаться. Улучшение оценивается как сумма вознаграждений, полученных за эпизод.\n",
    "    1. Используем $Q^{\\pi}(s,a;\\theta)$ для действий в среде, действуя жадно по отношению к $Q$-значениям. Сохраняем все прецеденты $(s,a,r,s')$.\n",
    "    2. Используем сохраненные прецеденты для обновления $Q^{\\pi}(s,a;\\theta)$ с помощью уравнения Беллмана в методе SARSA. Это улучшается оценку $Q$-функции,\n",
    "       что в свою очередь, совершенствует стратегию, так как оценки $Q$-значений теперь лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440d844-8d3e-4d89-a660-5135b78e6625",
   "metadata": {},
   "source": [
    "## Исследование и использование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361888f-37d0-40b4-a93c-48310b0e376c",
   "metadata": {},
   "source": [
    "Была доказана сходимость TD-обучения к оптимальной $Q$-функции для линейной аппроксимации функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0737b-14e8-4805-b3e7-92e8b9324200",
   "metadata": {},
   "source": [
    "- Sutton R. S. Learning to predict by the methods of temporal differences // Machine Learning. 1988. Vol. 3 (1), P. 9–44. DOI: [10.1007/BF00115009](https://doi.org/10.1007/BF00115009).\n",
    "- Tsitsiklis J. N. An Analysis of Temporal-Difference Learning with Function Approximation // IEEE Transactions on Automatic Control. 1997. Vol. 42 (5). P. 674–690. DOI: [10.1109/9.580874](https://doi.org/10.1109/9.580874)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73063140-a5ee-4fd1-adcf-0a0a0e0e7440",
   "metadata": {},
   "source": [
    "Сходимость TD-обучения при переходе к нелинейной аппроксимации функции не гарантируется.\n",
    "\n",
    "- https://www.youtube.com/watch?v=k1vNh4rNYec\n",
    "- Mnih V., Kavukcuoglu K., Silver D., Graves A., Antonoglou I., Wierstra D. Riedmiller M. A. Playing Atari with Deep Reinforcement Learning. 2013. https://arxiv.org/abs/1312.5602\n",
    "- Tesauro G. Temporal Difference Learning and TD-Gammon // Communications of the ACM 38.3\n",
    "    - https://github.com/AestheticVoyager/Temporal-Difference-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc3d70-a472-464e-a1a0-e35e1ab1fba8",
   "metadata": {},
   "source": [
    "## Алгоритм SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e79fc-0217-4e53-bbab-18fc78de4b5d",
   "metadata": {},
   "source": [
    "Оценка $Q$-функции $\\hat Q^{\\pi}(s,a)$ параметризована сетью с параметрами $\\theta$, обозначенной как $Q^{\\pi_{\\theta}}$, так что $\\hat Q^{\\pi}(s,a)=Q^{\\pi_{\\theta}}(s,a).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b8382-0ede-4d30-9dad-9dd686cc4c8e",
   "metadata": {},
   "source": [
    "1. Инициализация скорости обучения $\\alpha$.\n",
    "2. Инициализация $\\varepsilon$.\n",
    "3. Инициализация параметров сети $\\theta$ случайными значениями.\n",
    "4. **for** $m=1,2,\\ldots, MAX\\_STEPS$ **do**\n",
    "5. $\\quad$ Накопить $N$ прецедентов $(s_i,a_i,r_i,s_i',a_i')$, используя текущую $\\varepsilon$-жадную стратегию.\n",
    "6. $\\quad$ **for** $i = 1,2,\\ldots, N$ **do**\n",
    "7. $\\quad\\quad$ # Расситать целевые $Q$-значения для каждого прецедента\n",
    "8. $\\quad\\quad$ $y_i=r_i+\\delta_{s_i'}\\gamma Q^{\\pi_{\\theta}}(s_i',a_i')$, где $\\delta_{s_i'}=\\theta$, если $s_i'$ — конечное состояние, иначе 1\n",
    "9. $\\quad$ **end for**\n",
    "10. $\\quad$ # Расчет функции потерь, например, с помощью скп\n",
    "11. $\\quad$ $L(\\theta)=\\dfrac{1}{N}\\sum_{i}\\left(y_i-Q^{\\pi_{\\theta}}(s_i,a_i)\\right)^2$\n",
    "12. $\\quad$ # Обновление параметров сети\n",
    "13. $\\quad$ $\\theta=\\theta - \\alpha \\nabla_{\\theta} J(\\theta)$\n",
    "14. $\\quad$ Уменьшение $\\varepsilon$\n",
    "15. **end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c0328-110a-4613-8206-b85cf338e0b3",
   "metadata": {},
   "source": [
    "## Алгоритм обучения по актуальному опыту"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e3fea-7ea8-4e59-82f6-9ece86c3c774",
   "metadata": {},
   "source": [
    "Рассмотрим TD-обновление в SARSA\n",
    "$$\n",
    "Q^{\\pi_1}(s,a)\\approx r + \\gamma Q^{\\pi_1}\\left(s',a_1'\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c0b0f-bd32-4f6a-a75b-0cc497dd239c",
   "metadata": {},
   "source": [
    "Здесь неуместны прецеденты, сгенерированные с помощью других стратегий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e69e2-009b-478c-a6cf-d9a13d583502",
   "metadata": {},
   "source": [
    "Пусть существует прецедент $(s,a,r,s',a'_2)$, порожденный стратегией $\\pi_2$.\n",
    "\n",
    "$a'_2$ не обязательно будет таким же, как $a'_1$.\n",
    "Если это так, то $Q^{\\pi_1}(s,a)$ не будет отражать ожидаемое суммарное будущее дисконтированное вознаграждение за выбор действия $a$ в состоянии $s$ в соответствии с $\\pi_1$:\n",
    "$$\n",
    "Q^{\\pi_1}_{\\text{tar}}(s,a)=r+\\gamma Q^{\\pi_1}(s',a'_1)\\neq r + \\gamma Q^{\\pi_1}(s',a'_2).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe91ecb-8bb8-4ae0-aca8-620109ea3856",
   "metadata": {},
   "source": [
    "## Реализация SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec76175-b47b-4c9d-b4e1-84b80e08e932",
   "metadata": {},
   "source": [
    "Реализация SARSA состоит из трех основных компонентов.\n",
    "\n",
    "- epsilon_gredy определяет метод для действий в среде согласно текущей стратегии.\n",
    "- calc_q_loss использует накопленные прецеденты для расчета целевых $Q$-значений и соответствующих значений функции потерь.\n",
    "  $$\n",
    "  \\hat Q^{\\pi}(s,a)=r+\\gamma\\hat Q^{\\pi}(s',a')=Q^{\\pi}_{\\text{tar}}(s,a).\n",
    "  $$\n",
    "- train производит одно обновление параметров сети полезности с помощью $Q$-функции потерь. Происходит обновление всех значимых переменных, таких как explore_var($\\varepsilon$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d18ce2-08b0-477e-a3b4-7d09689328e1",
   "metadata": {},
   "source": [
    "См. реализацию из SLM-Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
