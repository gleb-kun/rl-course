{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba32e87d-e9f4-40cc-a6be-c89d5f5173d3",
   "metadata": {},
   "source": [
    "# Лекция 03. Алгоритмы, основанные на стратегиях и полезностях. Алгоритм SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e1a19-ca67-4541-9632-3af72dc25a1b",
   "metadata": {},
   "source": [
    "## State — Action — Reward — State — Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f46d98-a48e-46e3-83af-e3cff42a97c5",
   "metadata": {},
   "source": [
    "<img src=\"pictures/niranjan.jpg\" alt=\"Mahesan Niranjan\" width=\"300\">\n",
    "\n",
    "Махесан Ниранджан (Mahesan Niranjan)\n",
    "\n",
    "- Rummery G. A., Niranjan M. On-line $Q$-learning using connectionistsystems. Technical Report. University of Cambrige. 1994. URL: https://www.researchgate.net/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems\n",
    "\n",
    "<img src=\"pictures/rummery.jpg\" alt=\"Rummery G. A.\" width=\"300\">\n",
    "\n",
    "Руммери Г. А. (Rummery G. A.) ???\n",
    "\n",
    "https://www.researchgate.net/scientific-contributions/G-A-Rummery-6612457\n",
    "\n",
    "<img src=\"pictures/sutton.jpg\" alt=\"Richard Satton\" width=\"300\">\n",
    "\n",
    "Ричард Саттон (Richard S. Sutton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c971e-c3b6-4ad6-88a1-f5f03d721be6",
   "metadata": {},
   "source": [
    "## $Q$-функция и $V$-функция"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c93d2-9673-45d1-adf7-398044f9bd54",
   "metadata": {},
   "source": [
    "$$\n",
    "Q^{\\pi}\\left(s,a\\right)=\\mathbb{E}_{s_0=s,\\, a_0=a,\\, \\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^{t}r_t\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584ede3-af89-4fc9-b9a5-2d87b998983b",
   "metadata": {},
   "source": [
    "$$\n",
    "V^{\\pi}\\left(s\\right)=\\mathbb{E}_{s_0=s,\\,\\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^{t}r_t\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce67a7f-efa9-4c57-9135-4e3c938ee514",
   "metadata": {},
   "source": [
    "$Q^{\\pi}\\left(s,a\\right)$ тсвязана с $V^{\\pi}\\left(s\\right)$, которая является мат ожиданием $Q$-значений для всех действий $a$, доступных на отдельно взятом состоянии $s$ при стратегии $\\pi$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592bb14-1583-4bab-a140-82fa3043ab72",
   "metadata": {},
   "source": [
    "$$\n",
    "V^{\\pi}\\left(s\\right)=\\mathbb{E}_{a\\sim\\pi\\left(s\\right)}\\left[Q^{\\pi}(s,a)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79b638-1d86-4c46-9760-3383e2355adc",
   "metadata": {},
   "source": [
    "Какую же функцию лучше настраивать агенту??? $Q^{\\pi}\\left(s,a\\right)$ или $V^{\\pi}(s)$???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfff8ad-1728-4fb6-b4b8-abe327d5b708",
   "metadata": {},
   "source": [
    "**Пример.** Предположим, что выбор действия $a$ в соостоянии $s$ приводит к:\n",
    "- состоянию $s'_1$ с вознаграждением $r_1'=1$,\n",
    "- состоянию $s'_2$ с вознаграждением $r_2'=2$.\n",
    "\n",
    "Также известны полезности для $s'_1$ и $s'_2$, $V^{\\pi}\\left(s_1'\\right)=10$ и $$V^{\\pi}\\left(s_2'\\right)=-10,$$ тогда\n",
    "$$\n",
    "r_1'+V^{\\pi}\\left(s_1'\\right)=1+10=11,\n",
    "$$\n",
    "$$\n",
    "r_2'+V^{\\pi}\\left(s_2'\\right)=2-10=-8.\n",
    "$$\n",
    "\n",
    "Тогда из уравнения\n",
    "$$\n",
    "\\mathbb{E}\\left[r+V^{\\pi}\\left(s'\\right)\\right]=\\frac{1}{2}\\left(r_1'+V^{\\pi}\\left(s'_1\\right)+r_2'+V^{\\pi}\\left(s'_2\\right)\\right)=\\frac{1}{2}\\left(11-8\\right)=1.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a1361-fddb-4011-947b-4a5d837862b2",
   "metadata": {},
   "source": [
    "## Метод временных различий (TD-обучение)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd15908-4c4a-46d4-a497-40ae7f65f522",
   "metadata": {},
   "source": [
    "В SARSA целевые значения $Q_{\\text{tar}}$ порождаются с помощью TD-обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5891a17-2171-4f20-a9bb-8d7e44c607d4",
   "metadata": {},
   "source": [
    "Основная суть использования метода — использование нейронной сети (**сети полезностей**, **value network**), которая на входе оценивает $Q$-значения заданных пар $(s,a)$.\n",
    "\n",
    "- Генерируются $\\tau$ и даются прогнозные $\\hat Q$-значения для $\\forall (s,a)$.\n",
    "- С помощью $\\tau$ генерируются целевые $Q$-значения $Q_{\\text{tar}}$.\n",
    "- Минимизируются расстояния между $\\hat Q$ и $Q_{\\text{tar}}$ с помощью стандартной регрессионной функции потреь, такой как скп.\n",
    "- Процесс повторяем многократно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f5749-efa8-4f74-b84b-deedcc1f0754",
   "metadata": {},
   "source": [
    "### Почему не метод Монте-Карло?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b56874-57d0-46c1-86ce-f103d1d9948c",
   "metadata": {},
   "source": [
    "Пусть даны $N$ траекторий $\\tau_i, i=1,2,\\ldots,N$, начиная с состояния $s$, в котором агент выбрал дейтвие $a$. Тогда оценка по методу Монте-Карло"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ae7e2-6fb0-4312-a59b-d56f1703dac2",
   "metadata": {},
   "source": [
    "$$Q^{\\pi}_{\\text{tar:MC}(s, a)}=\\frac{1}{N}\\sum_{i=1}^{N}R(\\tau_{i}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541072f-38d2-4179-98e2-f5ed20a4eb69",
   "metadata": {},
   "source": [
    "Отметим, что это уравнение применимо к любому набору траекторий с любой начальной парой $(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6000f5-79d4-45df-bb02-d7d67f6b951f",
   "metadata": {},
   "source": [
    "- Агенту приходится ждать конца эпизода, прежде чем можно будет воспользоваться для обучения какими-нибудь данными из него.\n",
    "- Для расчета $Q^{\\pi}_{\\text{tar:MC}(s, a)}$ нужны вознаграждения для остальной части траектории, начиная с $(s,a)$. Эпизоды могут состоять из большого количества временных шагов, ограниченного $T$, что задерживает обучение. Этим обучловлено использование TD-обучения для настройки $Q$-значений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7ab72-fedf-43ec-9896-96cee105b790",
   "metadata": {},
   "source": [
    "### TD-обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0853ccd-c0a4-41e3-ad8e-7087e18695a5",
   "metadata": {},
   "source": [
    "$Q$-значения для текущего временного шага могут быть определены через $Q$-значения для следующего временного шага (рекурсивно):\n",
    "$$\n",
    "Q^{\\pi}(s,a)=\\mathbb{E}_{s'\\sim p(s'| s, a),\\, r\\sim \\mathcal R (s,a, s')}\\left[r+\\gamma\\mathbb{E}_{a'\\sim\\pi(s')}\\left[Q^{\\pi}(s',a')\\right]\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfe209-814a-491d-9018-78c87348e7a1",
   "metadata": {},
   "source": [
    "<img src=\"pictures/bellman.jpg\" alt=\"Richard E. Bellman\" width=\"300\">\n",
    "\n",
    "Ричард Эрнест Беллман (Richard E. Bellman)\n",
    "\n",
    "(1920–1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930bcfd-838d-4271-ac34-8e337c783271",
   "metadata": {},
   "source": [
    "Пусть существует нейронная сеть $Q_{\\theta}$ представляющая $Q$-функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440d844-8d3e-4d89-a660-5135b78e6625",
   "metadata": {},
   "source": [
    "## Исследование и использование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361888f-37d0-40b4-a93c-48310b0e376c",
   "metadata": {},
   "source": [
    "Была доказана сходимость TD-обучения к оптимальной $Q$-функции для линейной аппроксимации функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0737b-14e8-4805-b3e7-92e8b9324200",
   "metadata": {},
   "source": [
    "- Sutton R. S. Learning to predict by the methods of temporal differences // Machine Learning. 1988. Vol. 3 (1), P. 9–44. DOI: [10.1007/BF00115009](https://doi.org/10.1007/BF00115009).\n",
    "- Tsitsiklis J. N. An Analysis of Temporal-Difference Learning with Function Approximation // IEEE Transactions on Automatic Control. 1997. Vol. 42 (5). P. 674–690. DOI: [10.1109/9.580874](https://doi.org/10.1109/9.580874)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73063140-a5ee-4fd1-adcf-0a0a0e0e7440",
   "metadata": {},
   "source": [
    "Сходимость TD-обучения при переходе к нелинейной аппроксимации функции не гарантируется.\n",
    "\n",
    "- https://www.youtube.com/watch?v=k1vNh4rNYec\n",
    "- Mnih V., Kavukcuoglu K., Silver D., Graves A., Antonoglou I., Wierstra D. Riedmiller M. A. Playing Atari with Deep Reinforcement Learning. 2013. https://arxiv.org/abs/1312.5602\n",
    "- Tesauro G. Temporal Difference Learning and TD-Gammon // Communications of the ACM 38.3\n",
    "    - https://github.com/AestheticVoyager/Temporal-Difference-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc3d70-a472-464e-a1a0-e35e1ab1fba8",
   "metadata": {},
   "source": [
    "## Алгоритм SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b8382-0ede-4d30-9dad-9dd686cc4c8e",
   "metadata": {},
   "source": [
    "1. Инициализация скорости обучения $\\alpha$.\n",
    "2. Инициализация $\\varepsilon$.\n",
    "3. Инициализация параметров сети $\\theta$ случайными значениями.\n",
    "4. **for** $m=1,2,\\ldots, MAX\\_STEPS$ **do**\n",
    "5. $\\quad$ Накопить $N$ прецедентов $(s_i,a_i,r_i,s_i',a_i')$, используя текущую $\\varepsilon$-жадную стратегию.\n",
    "6. $\\quad$ **for** $i = 1,2,\\ldots, N$ **do**\n",
    "7. $\\quad\\quad$ # Расситать целевые $Q$-значения для каждого прецедента\n",
    "8. $\\quad\\quad$ $y_i=r_i+\\delta_{s_i'}\\gamma Q^{\\pi_{\\theta}}(s_i',a_i')$, где $\\delta_{s_i'}=\\theta$, если $s_i'$ — конечное состояние, иначе 1\n",
    "9. $\\quad$ **end for**\n",
    "10. $\\quad$ # Расчет функции потерь, например, с помощью скп\n",
    "11. $\\quad$ $L(\\theta)=\\dfrac{1}{N}\\sum_{i}\\left(y_i-Q^{\\pi_{\\theta}}(s_i,a_i)\\right)^2$\n",
    "12. $\\quad$ # Обновление параметров сети\n",
    "13. $\\quad$ $\\theta=\\theta - \\alpha \\Delta_{\\theta} J(\\theta)$\n",
    "14. $\\quad$ Уменьшение $\\varepsilon$\n",
    "15. **end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c0328-110a-4613-8206-b85cf338e0b3",
   "metadata": {},
   "source": [
    "## Алгоритм обучения по актуальному опыту"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e3fea-7ea8-4e59-82f6-9ece86c3c774",
   "metadata": {},
   "source": [
    "Рассмотрим TD-обновление в SARSA\n",
    "$$\n",
    "Q^{\\pi_1}(s,a)\\approx r + \\gamma Q^{\\pi_1}\\left(s',a_1'\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c0b0f-bd32-4f6a-a75b-0cc497dd239c",
   "metadata": {},
   "source": [
    "Здесь неуместны прецеденты, сгенерированные с помощью других стратегий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e69e2-009b-478c-a6cf-d9a13d583502",
   "metadata": {},
   "source": [
    "Пусть существует прецедент $(s,a,r,s',a'_2)$, порожденный стратегией $\\pi_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe91ecb-8bb8-4ae0-aca8-620109ea3856",
   "metadata": {},
   "source": [
    "## Реализация SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec76175-b47b-4c9d-b4e1-84b80e08e932",
   "metadata": {},
   "source": [
    "Реализация SARSA состоит из трех основных компонентов.\n",
    "\n",
    "- epsilon_gredy определяет метод для действий в среде согласно текущей стратегии.\n",
    "- calc_q_loss использует накопленные прецеденты для расчета целевых $Q$-значений и соответствующих значений функции потерь.\n",
    "- train производит одно обновление параметров сети полезности с помощью $Q$-функции потерь. Происходит обновление всех значимых переменных, таких как explore_var($\\varepsilon$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
