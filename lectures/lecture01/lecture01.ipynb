{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7222ba2a-088e-4225-b076-7677ec12e9d3",
   "metadata": {},
   "source": [
    "# Лекция 01. Введение в обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4d9d-e88f-489a-bab0-55bbdff49b54",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "\n",
    "Обучение с подкреплением (reinforcement learning, RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e15b2-e7c7-4e50-8de4-dc2a69db012a",
   "metadata": {},
   "source": [
    "### Фреймворк SLM-lab\n",
    "\n",
    "- Официальный репозиторий: `https://github.com/kengz/SLM-Lab`\n",
    "- База знаний: `https://slm-lab.gitbook.io/slm-lab`\n",
    "- Установка: `https://slm-lab.gitbook.io/slm-lab/setup/installation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fee8fc-65fe-4336-b050-1121baa6b2fa",
   "metadata": {},
   "source": [
    "### Цикл управления агентом в обучении с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513bcdc-6550-491c-bbe9-4ccbbe81b870",
   "metadata": {},
   "source": [
    "Обучение с подкреплением (англ. reinforcement learning) — это подход в машинном обучении, при котором агент учится, взаимодействуя с окружающей средой.\n",
    "\n",
    "![](../../common/pictures/result/environment.png)\n",
    "\n",
    "Система обучения с подкреплением реализует цикл управления с обратной связью, где агент и среда взаимодействуют и обмениваются сигналами, причем агент пытается максимизировать целевую функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca939ce-45a3-4c36-898a-4e4d88691b48",
   "metadata": {},
   "source": [
    "Введем следующие обозначения.\n",
    "\n",
    "**Прецедент**: сигналы (тройка (кортеж))\n",
    "$$\n",
    "(s_t, a_t, r_t),\n",
    "$$\n",
    "что соответствует состоянию, действию и вознаграждению. Индекс $t=0\\ldots T$ — момент времени (шаг).\n",
    "\n",
    "**Эпизод**:\n",
    "Временной горизонт от $t=0$ до момента завершения среды.\n",
    "\n",
    "**Траектория**:\n",
    "Последовательность прецедентов, или часть опыта, накопленного в течение эпизода:\n",
    "$$\n",
    "\\tau=(s_0,a_0,r_0),(s_1,a_1,r_1),\\ldots\n",
    "$$\n",
    "\n",
    "**Состояние**:\n",
    "\\begin{equation}\n",
    "    s_t\\in S,\n",
    "\\end{equation}\n",
    "где $S$ — пространство состояний, это набор всех возможных в среде состояний.\n",
    "\n",
    "**Действие**:\n",
    "\\begin{equation}\n",
    "    a_t\\in A,\n",
    "\\end{equation}\n",
    "где $A$ — пространство действий.\n",
    "\n",
    "**Вознаграждение**:\n",
    "\\begin{equation}\n",
    "    r_t=R\\left(s_t,a_t,s_{t+1}\\right),\n",
    "\\end{equation}\n",
    "где $R$ — функция вознаграждения.\n",
    "\t\n",
    "Пространства $S, A$ и функция $R$ составляют кортежи $\\left(s, a, r\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6343eba-beb5-47cf-a80d-ce23eb5a3322",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением как МППР\n",
    "\n",
    "### Функция переходов\n",
    "\n",
    "Рассмотрим как среда переходит из одного состояния в другое. Для этого введем **функцию переходов**. В обучении с подкреплением это компонент марковского процесса принятия решений (МППР).\n",
    "\n",
    "Рассмотрим общую постановку задачи:\n",
    "$$\n",
    "s_{t+1}\\sim P\\left(s_{t+1}|\\left(s_0,a_0\\right),\\left(s_1,a_1\\right),\\ldots,\\left(s_t,a_t\\right)\\right).\n",
    "$$\n",
    "Тут утверждается, что на временном шаге $t$ следующее состояние $s_{t+1}$ берется из распределения вероятностей $P$, обусловенного всей историей. Вероятность перехода среды из состояния $s_t$ в состояние $s_{t+1}$ зависит от всех предыдущих состояний $s$ и действий $a$, которые имели место в данном эпизоде до этого момента. Такая постановка трудно реализуема на практике.\n",
    "\n",
    "Преобразуем ее в МППР. Сделаем предположение, что переход в следующее состояние $s_{t+1}$ зависит только от предыдущих значений состояния $s_t$ и действия $a_t$, которое известно как **марковское свойство**. С учетом этого функция переходов примет вид\n",
    "$$\n",
    "s_{t+1}\\sim P\\left(s_{t+1}|s_t,a_t\\right).\n",
    "$$\n",
    "Тут утверждается, что следующее состояние $s_{t+1}$ берется из распределения вероятностей $P\\left(s_{t+1}|s_t,a_t\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d748585-ca96-4499-b66e-6c1fb0674300",
   "metadata": {},
   "source": [
    "### МППР\n",
    "\n",
    "Представим задачу обучения с подкреплением в виде МППР. МППР определяется кортежем из четырех элементов $S, A, P(.), \\mathcal R(.)$:\n",
    "\n",
    "- $S$ — набор состояний;\n",
    "- $A$ — набор действий;\n",
    "- $P(s_{t+1}|s_t,a_t)$ — функция перехода состояний среды;\n",
    "- $\\mathcal R(s_t,a_t,s_{t+1})$ — функция вознаграждения среды.\n",
    "\n",
    "**Замечание.** Функция переходов $P(s_{t+1}|s_t,a_t)$ и функция вознаграждений $\\mathcal R(s_t,a_t,s_{t+1})$ недоступны для агентов. Агенты могут получать информацию об этих функциях только через состояния, действия и вознаграждения, воздействию которых подвергаются на данный момент в среде, то есть через кортежи $(s_t,a_t, r_t)$.\n",
    "\n",
    "Чтобы формулировка задачи была полной, нужно формализовать понятие максимизируемой агентом целевой функции. Во-первых, опеределим **отдачу** $R(\\tau)$, используя траекторию из эпизода $\\tau=(s_0,a_0,r_0)\\ldots(s_T,a_T,r_T)$:\n",
    "$$\n",
    "R(\\tau)=r_0+\\gamma r_1 +\\gamma^2r_2+\\ldots+\\gamma^T r_T=\\sum_{t=0}^{T}\\gamma^t r_t.\n",
    "$$\n",
    "Здесь отдача определена как дисконтированная сумма вознаграждений на траектории, где $\\gamma$ — коэффициент дисконтирования, $\\gamma=[0,1]$.\n",
    "\n",
    "Тогда **целевая функция** $J(\\tau)$ становится просто математическим ожиданием отдачи по нескольким траекториям:\n",
    "$$\n",
    "J(\\tau)=\\mathbb{E}_{\\tau\\sim\\pi}\\left[R(\\tau)\\right]=\\mathbb{E}_\\tau\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right].\n",
    "$$\n",
    "\n",
    "Отдача $R(\\tau)$ —  это сумма дисконтированнных вознаграждений $\\gamma^{t}r_t$ за все временные шаги $t=0,\\ldots,T$. Целевая функция $J(\\tau)$ — это отдача, усредненная по нескольким эпизодам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b4f3a-b1a8-4ce6-8608-e52213aa92fe",
   "metadata": {},
   "source": [
    "#### Цикл управления МППР"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392be658-76b7-474d-a643-ffa878b14290",
   "metadata": {},
   "source": [
    "    Считаем, что агент и среда env уже созданы\n",
    "    for episode = 0,...,MAX_EPISODE do\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        for t = 0,...,T do\n",
    "            action = agent.act(state)\n",
    "            state, reward = env.step(action)\n",
    "            agent.update(action, state, reward)\n",
    "            if env.done() then\n",
    "                break\n",
    "            end if\n",
    "        end for\n",
    "    end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed8d91-34ec-4a07-9c12-563601ab7dfc",
   "metadata": {},
   "source": [
    "## Обучение функции в обучении с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef0574-1b36-46c5-a89b-e39497facfb3",
   "metadata": {},
   "source": [
    "Обозначим кортежи $(s_t,a_t,r_t), (s_{t+1},a_{t+1},r_{t+1})$ как $(s,a,r), (s',a',r')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad40b1f-760a-45f6-a7aa-84131bc8b453",
   "metadata": {},
   "source": [
    "Есть три основные функции, которые изучаются в обучении с подкреплением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5b4e7-adf2-4880-8caa-af57d7f5200c",
   "metadata": {},
   "source": [
    "1. Стратегия $\\pi$, которая сопоставляет состоянию действие $a \\sim \\pi\\left(s\\right)$.\n",
    "2. Функция полезности $V^{\\pi}\\left(s\\right)$ или $Q^{\\pi}\\left(s,a\\right)$ для вычисления ожидаемой отдачи $\\mathbb{E}\\left[R(\\tau)\\right]:$\n",
    "   $$V^{\\pi}(s)=\\mathbb{E}_{s_0=s,\\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right],$$\n",
    "   $$Q^{\\pi}(s,a)=\\mathbb{E}_{s_0=s,a_0=a,\\tau\\sim\\pi}\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right].$$\n",
    "4. Модель среды $P\\left(s'| s, a\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df67a3-236f-40a9-a499-5bfed1c63114",
   "metadata": {},
   "source": [
    "![](../../common/pictures/result/cell-ex.png)\n",
    "\n",
    "Вознаграждения $r$ и полезности $V^{\\pi}(s)$ для каждого состояния $s$ в простой клеточной среде. Полезность состояния рассчитывается по вознаграждениям при $\\gamma=0.9$. Здесь применяется стратегия $\\pi$, при которой всегда выбирается кратчайший путь к целевому состоянию с $r=+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498f5d8-a8d9-47f1-8414-7f49e4a3e7f7",
   "metadata": {},
   "source": [
    "## Обзор главных алгоритмов глубокого обучения с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9493fc-b586-4acd-9bad-ab88a6019776",
   "metadata": {},
   "source": [
    "![](../../common/pictures/result/algorithms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
