{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d248407-c59a-4ca7-bbfc-743e389bda03",
   "metadata": {},
   "source": [
    "# Лекция 00. Глубокое обучение с подкреплением. О чем курс?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9234ff8-3b12-4021-9495-d4321f7eaaa9",
   "metadata": {},
   "source": [
    "## План на сегодня\n",
    "\n",
    "- О чем курс?\n",
    "    - Глубокое обучение с подкреплением\n",
    "    - Формализация\n",
    "    - Приложение\n",
    "- Литература\n",
    "- Иллюстрации на Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce104c-adab-42c2-bb8c-6e510166fc52",
   "metadata": {},
   "source": [
    "## Так о чем курс?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c593b3-36c8-4b9c-83f8-508e96f13dfa",
   "metadata": {},
   "source": [
    "### Определение\n",
    "- Обучение с подкреплением\n",
    "- Глубокое обучение с подкреплением\n",
    "\n",
    "Обучение с подкреплением (англ. reinforcement learning) — это подход в машинном обучении, при котором агент учится, взаимодействуя с окружающей средой.\n",
    "\n",
    "![](pictures/environment.png)\n",
    "\n",
    "В кибернетике обучение с подкреплением рассматривается как разновидность кибернетического эксперимента. Вместо того чтобы использовать специально заданную систему управления обучением (как в обучении с учителем), агент получает обратную связь от среды в форме сигналов подкрепления. Таким образом, среда или ее модель выполняет роль учителя. Отдельные методы обучения с подкреплением могут также включать неявных учителей, например, в искусственных нейронных системах, где подкрепление связано с одновременной активностью нейронов, что сближает эти методы с обучением без учителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d4d5-c9e2-4014-a0c1-d1d6661380d1",
   "metadata": {},
   "source": [
    "### Формализация обучения с подкрепления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e35a02-2186-4799-831a-4d3340a56730",
   "metadata": {},
   "source": [
    "#### Обучение с подкреплением как марковский процесс принятия решений (МППР)\n",
    "\n",
    "Среда описывается как пятерка: $M = (S, A, P, R, \\gamma)$,\n",
    "где:\n",
    "- $S$ — множество состояний среды;\n",
    "- $A$ — множество действий, которые агент может выполнять;\n",
    "- $P$ — вероятность перехода в состояние $s'$, если агент находился в состоянии $s$ и выбрал действие $a$;\n",
    "- $R(s, a)$ — функция вознаграждения, возвращающая сигнал подкрепления за выполнение действия $a$ в состоянии $s$;\n",
    "- $\\gamma \\in [0, 1]$ — коэффициент дисконтирования, учитывающий влияние будущих вознаграждений.\n",
    "\n",
    "#### Функция полезности (value function)\n",
    "\n",
    "##### Функция ценности состояния (state-value function):\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right],$$\n",
    "где $\\pi(a \\mid s)$ — стратегия, определяющая вероятность выбора действия $a$ в состоянии $s$.\n",
    "\n",
    "##### Функция ценности действия (action-value function):\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a \\right]$$.\n",
    "\n",
    "#### Оптимальная стратегия\n",
    "\n",
    "Оптимальная стратегия $\\pi^*$ максимизирует ожидаемое суммарное вознаграждение:\n",
    "$$\n",
    "\\pi^* = \\arg\\max_\\pi V^\\pi(s), \\quad \\forall s \\in S.\n",
    "$$\n",
    "\n",
    "Оптимальные функции ценности определяются следующим образом:\n",
    "- Оптимальная функция ценности состояния:\n",
    "    $$\n",
    "    V^*(s) = \\max_a Q^*(s, a).\n",
    "    $$\n",
    "- Оптимальная функция ценности действия:\n",
    "    $$\n",
    "    Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\max_{a'} Q^*(s', a').\n",
    "    $$\n",
    "\n",
    "#### Обновление стратегии\n",
    "\n",
    "Алгоритмы обучения с подкреплением, такие как $Q$-learning, используют итеративное обновление функции $Q$ для приближения оптимального значения:\n",
    "$$\n",
    "Q(s, a) \\gets Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right],\n",
    "$$\n",
    "где $\\alpha$ — скорость обучения, а $s'$ — новое состояние после выполнения действия $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1e9a2-4c5d-4b2a-93a5-b7f7f2a08191",
   "metadata": {},
   "source": [
    "### Приложение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b7c9f-3d69-483b-a657-f9e01337aa44",
   "metadata": {},
   "source": [
    "#### Динамические системы (минимальные определения, чтобы вкурить пример)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf56c1c-e2b1-4d8c-928a-a313af45f02d",
   "metadata": {},
   "source": [
    "Есть пара $\\left(X, \\mathcal L\\right)$ — динамическая система с $n$ степенями свободы.\n",
    "- $X$ — конфигурационное пространство,\n",
    "- $\\mathcal L = \\mathcal L\\left(t,q(t), v(t)\\right)$ — лагранжиан. Такая вещественная гладка функция, что $q(t)\\in X$ и $v(t)$ — $n$-мерный вещественный вектор.\n",
    "\n",
    "Пусть $P(a, b, x_a, x_b)$ — множество гладких путей  \n",
    "$q : [a, b] → X$, для которых выполняются условия:  \n",
    "$q(a) = x_a$ и $q(b) = x_b$.\n",
    "\n",
    "Функционал действия $S : P(a, b, x_a, x_b) → ℝ$  \n",
    "определяется выражением:\n",
    "\n",
    "$$\n",
    "S[q] = \\int_a^b L\\big(t, q(t), \\dot{q}(t)\\big) \\, dt,\n",
    "$$\n",
    "\n",
    "где $L(t, q(t), \\dot{q}(t))$ — функция Лагранжа.  \n",
    "\n",
    "Путь $q ∈ P(a, b, x_a, x_b)$ является стационарной точкой $S$  \n",
    "тогда и только тогда, когда выполняется уравнение:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial q^i}\\big(t, q(t), \\dot{q}(t)\\big) - \n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{q}^i}\\big(t, q(t), \\dot{q}(t)\\big) = 0, \\quad i = 1, \\dots, n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269d281-e669-4786-ab61-78bcda3d3c0e",
   "metadata": {},
   "source": [
    "#### Вывод одномерного уравнения Эйлера-Лагранжа\n",
    "\n",
    "Можно посмотреть [тут](https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation#Statement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f691ff-ec27-48cd-93fa-f7d8f8a5e762",
   "metadata": {},
   "source": [
    "Пример динамической системы с $n$ степенями свободы можно найти в классической механике. Рассмотрим материальную точку, движущуюся в трехмерном пространстве под действием силы тяжести.\n",
    "\n",
    "Параметры:\n",
    "- Конфигурационное пространство $X$ — это пространство всех возможных положений системы. В данном случае материальная точка может занимать любое положение в трехмерном евклидовом пространстве, то есть\n",
    "$$X=\\mathbb R^3$$.\n",
    "- Лагранжиан в классической механике определяется как разность между кинетической энергией $T$ и потенциальной энергией $V$ системы:\n",
    "$$\\mathcal L=T-V.$$\n",
    "Для материальной точки с массой $m$, движущейся в поле тяжести, кинетическая энергия равна:\n",
    "$$T=\\frac{1}{2}mv^2,$$\n",
    "здесь $v=\\left\\|\\textbf{v}\\right\\|$ — скорость точки. Потенциальная энергия определяется как:\n",
    "$$V=mgz,$$\n",
    "где $z$ — высота точки над выбранным нулевым уровнем ($Oz$ направлена вверх).\n",
    "Таким образом лагранжиан для системы будет:\n",
    "$$\\mathcal{L}(t,q,v)=\\frac{1}{2}m\\left\\|\\textbf{v}\\right\\|^2-mgz,$$\n",
    "где $q=(x,y,z)\\in \\mathbb{R}^3$ — положение точки, а $v=(v_x,v_y,v_z)$ — ее скорость.\n",
    "\n",
    "Для описания движения системы используется принцип наименьшего действия. Подставив лагранжиан $\\mathcal L$ в уравнения Лагранжа:\n",
    "$$\n",
    "\\frac{d}{dt}\\frac{\\partial\\mathcal L}{dv_i}-\\frac{\\partial \\mathcal L}{\\partial q_i}=0,\n",
    "$$\n",
    "где $q_i$ — координаты $(x,y,z)$, получим уравнения движения для каждой координаты. Например, для $z$:\n",
    "$$\n",
    "m\\frac{d^2 z}{dt^2}=-mg,\n",
    "$$\n",
    "что приводит к классическому результату — свободное падение точки с ускорением $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa107e-f483-4071-901f-38ae378ccf77",
   "metadata": {},
   "source": [
    "<img src=\"pictures/cart-pole.png\" width=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7cc36-c770-48e2-82f5-3aa1f1cab046",
   "metadata": {},
   "source": [
    "На схеме выше показано, как выглядит система. Предполагается, что тележка находится на фиксированном линейном рельсе (без трения). Маятник закреплен на тележке с помощью вращательного сочленения, которое имеет одну вращательную степень свободы. Существует несколько способов моделирования маятника. Некоторые моделируют его как однородный стержень/цилиндр, а некоторые — как сосредоточенную массу на конце безмассового стержня. Здесь он моделируется как тонкий цилиндрический стержень.\n",
    "\n",
    "Чтобы применить лагранжеву механику к системе тележка-шест, нужно сначала записать лагранжиан системы, который определяется как разность между потенциальной и кинетической энергиями системы как функциями состояния.\n",
    "\n",
    "Поскольку тележка движется перпендикулярно гравитации, потенциальная энергия, обозначим ее как $V$\n",
    "зависит только от вертикального положения маятника, таким образом:\n",
    "\\begin{equation}\n",
    "V=-m_p gl \\cos\\theta.\n",
    "\\end{equation}\n",
    "\n",
    "Кинетическая энергия системы немного сложнее. Это сумма кинетических энергий тележки и шеста\n",
    "\\begin{equation}\n",
    "T=\\frac{1}{2}m_c\\dot{x}^2 + \\frac{1}{2}m_p\\left[\\left(\\dot{x}^2+l\\dot{\\theta}\\cos\\theta\\right)^2+\\left(l\\dot{\\theta}\\sin\\theta\\right)^{2}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Тогда лагранжиан имеет вид:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}=T-V.\n",
    "\\end{equation}\n",
    "\n",
    "В механике Лагранжа, поскольку система развивается с течением времени, величина, называемая действием, определяется как интеграл Лагранжа\n",
    "\\begin{equation}\n",
    "S=\\int_{t_1}^{t_2}\\mathcal{L}dt.\n",
    "\\end{equation}\n",
    "\n",
    "Согласно принципу наименьшего действия, динамика системы развивается таким образом, что эта величина — действие — минимизируется. Как только у меня есть действие, я могу использовать уравнения Эйлера-Лагранжа, чтобы найти уравнения движения системы\n",
    "\n",
    "\n",
    "Пример с тележкой взят из [этого источника](https://www.ashwinnarayan.com/post/cartpole-dynamics/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869166b4-4097-469b-b403-fca8cd55e13b",
   "metadata": {},
   "source": [
    "### Методы\n",
    "\n",
    "- Основанные на стратегии, полезности, модели среды или комбинированные методы в зависимоти от того, какую из трех основных функций обучения с подкреплением настраивает алгоритм.\n",
    "    - Алгоритмы основанные на стратегиях и полезностях\n",
    "    - Комбинированные методы\n",
    "- Основанные на модели и безмодельные в зависимости от того, использует алгоритм модель динамики переходов среды или нет.\n",
    "- Ведущие обучение по актуальному или отложенному опыту в зависимости от того, учится агент на данных, полученных с помощью только текущей стратегии или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2c420-b906-4f91-9732-6cd5e217517b",
   "metadata": {},
   "source": [
    "## Литература, источники, зависимости\n",
    "\n",
    "### Книги\n",
    "\n",
    "- Грессер Лаура, Кенг Ван Лун. Глубокое обучение с подкреплением: теория и практика на языке Python. СПб.: Питер, 2022. 416 с. [PDF].\n",
    "- GitBook фреймворка SLM-Lab. URL: https://slm-lab.gitbook.io/slm-lab\n",
    "- Жабко А. П. Лекции по динамическим системам. СПб.: Издательство. С.-Петерб. гос. ун-т, 2003.\n",
    "\n",
    "### Фреймворк SLM-Lab\n",
    "\n",
    "Получаем фреймворк. Быстрый старт, как должно работать по задумке:\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/kengz/SLM-Lab.git\n",
    "git checkout book\n",
    "./bin/setup\n",
    "```\n",
    "\n",
    "В связи с тем, что репозиторий сейчас обновляется редко (на момент написания этого конспекта, последнее обновление было 3 года назад), могут возникать некоторые сложности.\n",
    "\n",
    "#### Пример в ArchLinux\n",
    "\n",
    "Потребуется менеджер виртуальных сред [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html).\n",
    "\n",
    "Можно установить либо в ручную, либо из репозитория AUR.\n",
    "\n",
    "```shell\n",
    "yay -S python-conda\n",
    "```\n",
    "\n",
    "Берем форк с правками зависимостей.\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/gleb-kun/SLM-Lab.git\n",
    "git checkout book\n",
    "```\n",
    "\n",
    "И устанавливаем и запускаем виртуальное окружение.\n",
    "\n",
    "```shell\n",
    "conda env create -f environment.yml\n",
    "conda activate lab\n",
    "```\n",
    "\n",
    "Может возникнуть ситуация со следующей ошибкой: `ImportError: libpcre16.so.3: cannot open shared object file: No such file or directory`. Проблема в том, что используется в SLM-Lab используется зависимость roboschool версии 1.0.46, которая требует наличие динамической библиотеки libpcre16.so.3. В текущих версиях ArchLinux, как в моем случае (и, вероятно, в других, например Ubuntu), пакеты [pcre](https://archlinux.org/packages/core/x86_64/pcre/), [pcre2](https://archlinux.org/packages/core/x86_64/pcre2/) не предоставляют libpcre16.so.3.\n",
    "\n",
    "#### Docker\n",
    "\n",
    "Самый простой и безболезненный способ это использовать виртуальную машину.\n",
    "\n",
    "1. Создаем образ.\n",
    "\n",
    "```shell\n",
    "docker build -t kengz/slm_lab:latest -t kengz/slm_lab:v4.2.0 .\n",
    "```\n",
    "\n",
    "2. Ну и собственно запускаем.\n",
    "\n",
    "```shell\n",
    "docker run --rm -it kengz/slm_lab:v4.2.0\n",
    "```\n",
    "\n",
    "3. Для удобства можно организовать трансляцию вирутального монитора с помощью программы **x11vnc**.\n",
    "\n",
    "```shell\n",
    "Xvfb :99 -screen 0 1024x768x24 &\n",
    "export DISPLAY=:99\n",
    "x11vnc -display :99 -bg -nopw -listen 0.0.0.0 -xkb -forever\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16ff62-ab22-4969-9c1c-6a239b306617",
   "metadata": {},
   "source": [
    "## Иллюстрация из SLM-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04edf4-0e3e-4ba5-a953-497e9bca862a",
   "metadata": {},
   "source": [
    "### CartPole\n",
    "\n",
    "```shell\n",
    "python run_lab.py slm_lab/spec/demo.json dqn_cartpole dev\n",
    "```\n",
    "\n",
    "<img src=\"pictures/cart-pole-slm.png\" width=\"50%\"></img>\n",
    "Пример рендеринга \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
