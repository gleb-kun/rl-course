{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d248407-c59a-4ca7-bbfc-743e389bda03",
   "metadata": {},
   "source": [
    "# Лекция 0. Глубокое обучение с подкреплением. О чем курс?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9234ff8-3b12-4021-9495-d4321f7eaaa9",
   "metadata": {},
   "source": [
    "## План на сегодня\n",
    "\n",
    "- О чем курс?\n",
    "- Литература\n",
    "- Hello World на Python и C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce104c-adab-42c2-bb8c-6e510166fc52",
   "metadata": {},
   "source": [
    "## Так о чем курс?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c593b3-36c8-4b9c-83f8-508e96f13dfa",
   "metadata": {},
   "source": [
    "### Определение\n",
    "- Обучение с подкреплением\n",
    "- Глубокое обучение с подкреплением\n",
    "\n",
    "Обучение с подкреплением (англ. reinforcement learning) — это подход в машинном обучении, при котором агент учится, взаимодействуя с окружающей средой.\n",
    "\n",
    "![](pics/environment.png)\n",
    "\n",
    "В кибернетике обучение с подкреплением рассматривается как разновидность кибернетического эксперимента. Вместо того чтобы использовать специально заданную систему управления обучением (как в обучении с учителем), агент получает обратную связь от среды в форме сигналов подкрепления. Таким образом, среда или ее модель выполняет роль учителя. Отдельные методы обучения с подкреплением могут также включать неявных учителей, например, в искусственных нейронных системах, где подкрепление связано с одновременной активностью нейронов, что сближает эти методы с обучением без учителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d4d5-c9e2-4014-a0c1-d1d6661380d1",
   "metadata": {},
   "source": [
    "### Марковский процесс принятия решений\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e35a02-2186-4799-831a-4d3340a56730",
   "metadata": {},
   "source": [
    "#### Среда как марковский процесс принятия решений (МППР)\n",
    "\n",
    "Среда описывается как пятерка: $M = (S, A, P, R, \\gamma)$,\n",
    "где:\n",
    "- $S$ — множество состояний среды;\n",
    "- $A$ — множество действий, которые агент может выполнять;\n",
    "- $P$ — вероятность перехода в состояние $s'$, если агент находился в состоянии $s$ и выбрал действие $a$;\n",
    "- $R(s, a)$ — функция вознаграждения, возвращающая сигнал подкрепления за выполнение действия $a$ в состоянии $s$;\n",
    "- $\\gamma \\in [0, 1]$ — коэффициент дисконтирования, учитывающий влияние будущих вознаграждений.\n",
    "\n",
    "#### Функция полезности (value function)\n",
    "\n",
    "##### Функция ценности состояния (state-value function):\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right],$$\n",
    "где $\\pi(a \\mid s)$ — стратегия, определяющая вероятность выбора действия $a$ в состоянии $s$.\n",
    "\n",
    "##### Функция ценности действия (action-value function):\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a \\right]$$.\n",
    "\n",
    "#### Оптимальная стратегия\n",
    "\n",
    "Оптимальная стратегия $\\pi^*$ максимизирует ожидаемое суммарное вознаграждение:\n",
    "$$\n",
    "\\pi^* = \\arg\\max_\\pi V^\\pi(s), \\quad \\forall s \\in S.\n",
    "$$\n",
    "\n",
    "Оптимальные функции ценности определяются следующим образом:\n",
    "- Оптимальная функция ценности состояния:\n",
    "    $$\n",
    "    V^*(s) = \\max_a Q^*(s, a).\n",
    "    $$\n",
    "- Оптимальная функция ценности действия:\n",
    "    $$\n",
    "    Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\max_{a'} Q^*(s', a').\n",
    "    $$\n",
    "\n",
    "#### Обновление стратегии\n",
    "\n",
    "Алгоритмы обучения с подкреплением, такие как $Q$-learning, используют итеративное обновление функции $Q$ для приближения оптимального значения:\n",
    "$$\n",
    "Q(s, a) \\gets Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right],\n",
    "$$\n",
    "где $\\alpha$ — скорость обучения, а $s'$ — новое состояние после выполнения действия $a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869166b4-4097-469b-b403-fca8cd55e13b",
   "metadata": {},
   "source": [
    "### Методы\n",
    "\n",
    "- Основанные на стратегии, полезности, модели среды или комбинированные методы в зависимоти от того, какую из трех основных функций обучения с подкреплением настраивает алгоритм.\n",
    "    - Алгоритмы основанные на стратегиях и полезностях\n",
    "    - Комбинированные методы\n",
    "- Основанные на модели и безмодельные в зависимости от того, использует алгоритм модель динамики переходов среды или нет.\n",
    "- Ведущие обучение по актуальному или отложенному опыту в зависимости от того, учится агент на данных, полученных с помощью только текущей стратегии или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2c420-b906-4f91-9732-6cd5e217517b",
   "metadata": {},
   "source": [
    "### Литература\n",
    "- Грессер Лаура, Кенг Ван Лун. Глубокое обучение с подкреплением: теория и практика на языке Python. СПб.: Питер, 2022. 416 с. [PDF].\n",
    "\n",
    "#### Фреймворк SLM-Lab\n",
    "\n",
    "    git clone https://github.com/kengz/SLM-Lab.git\n",
    "    git checkout book\n",
    "    ./bin/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c9b04-22fa-48d9-93e1-f2ed4c691893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36477f02-c2d8-4828-87b6-7f6b124c79de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
